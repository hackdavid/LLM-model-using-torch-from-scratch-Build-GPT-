# LLM-model-using-torch-from-scratch-Build-GPT-
In this repo, i am going to build a LLM model from scratch using torch and also we will cover the transformer archticture 
<br>
After google published transformers architecture in 2017 in a paper called "Attention all you need" , Almost all model using this architecture like GPT, LLAMA,Bert etc.
so in this repo i will explain you how LLM models works and also implement the trasformers with each layer mention in paper .
This repo has 2 part - <br>
# part-1
    first i will show you normal LLM model with simple model that how LLM model predict the next word based on previous word .
# part-2
    i will implement transformer model with each layer and see how transformer will improved the normal LLM model

so lets start with part-1

# part-1 steps 
1. first install all the library that will be required like torch
2. get the dataset - in this repo i am using a open-source dataset as text file you can also download , url is given in notebook code
3. 
